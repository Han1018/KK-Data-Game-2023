{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "data_root = Path(\"../kkdata3/\")\n",
    "train_source = pd.read_parquet(data_root / \"label_train_source.parquet\")\n",
    "train_target = pd.read_parquet(data_root / \"label_train_target.parquet\")\n",
    "test_source = pd.read_parquet(data_root / \"label_test_source.parquet\")\n",
    "meta_song = pd.read_parquet(data_root / \"meta_song.parquet\")\n",
    "meta_song_composer = pd.read_parquet(data_root / \"meta_song_composer.parquet\")\n",
    "meta_song_genre = pd.read_parquet(data_root / \"meta_song_genre.parquet\")\n",
    "meta_song_lyricist = pd.read_parquet(data_root / \"meta_song_lyricist.parquet\")\n",
    "meta_song_producer = pd.read_parquet(data_root / \"meta_song_producer.parquet\")\n",
    "meta_song_titletext = pd.read_parquet(data_root / \"meta_song_titletext.parquet\")\n",
    "\n",
    "\n",
    "train_source.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source.sort_values([\"session_id\", \"listening_order\"], inplace=True)\n",
    "train_target.sort_values([\"session_id\", \"listening_order\"], inplace=True)\n",
    "# test_source.sort_values([\"session_id\", \"listening_order\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map song_id to song_index to save memory and speed up\n",
    "meta_song[\"song_index\"] = meta_song.index\n",
    "train_source = train_source.merge(\n",
    "    meta_song[[\"song_id\", \"song_index\"]], on=\"song_id\", how=\"left\"\n",
    ")\n",
    "train_target = train_target.merge(\n",
    "    meta_song[[\"song_id\", \"song_index\"]], on=\"song_id\", how=\"left\"\n",
    ")\n",
    "test_source = test_source.merge(\n",
    "    meta_song[[\"song_id\", \"song_index\"]], on=\"song_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "train_source[\"unix_played_at\"] = pd.to_datetime(\n",
    "    train_source[\"unix_played_at\"], unit=\"s\"\n",
    ")\n",
    "train_target[\"unix_played_at\"] = pd.to_datetime(\n",
    "    train_target[\"unix_played_at\"], unit=\"s\"\n",
    ")\n",
    "test_source[\"unix_played_at\"] = pd.to_datetime(test_source[\"unix_played_at\"], unit=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple train a ML model - 20 predict 1\n",
    "\n",
    "follow ramdomforest, language is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train data\n",
    "# add artist & language column\n",
    "_language = train_source.merge(\n",
    "    meta_song[[\"song_index\", \"language_id\"]], on=\"song_index\", how=\"left\"\n",
    ")\n",
    "_language_artist = _language.merge(\n",
    "    meta_song[[\"song_id\", \"artist_id\"]], on=\"song_id\", how=\"left\"\n",
    ")\n",
    "df_train_x = _language_artist\n",
    "\n",
    "# Calculate the play duration, set 20th song's play duration to NaN\n",
    "df_train_x[\"play_duration\"] = df_train_x.groupby(\"session_id\")[\"unix_played_at\"].diff()\n",
    "df_train_x[\"play_duration\"] = df_train_x[\"play_duration\"].shift(-1)\n",
    "\n",
    "# check each session has 20 songs\n",
    "print(\n",
    "    \"check session has 20 songs:\",\n",
    "    (df_train_x[\"listening_order\"] == 20).sum() == len(df_train_x) // 20,\n",
    ")\n",
    "df_train_x = df_train_x.drop(columns=[\"song_id\", \"login_type\", \"unix_played_at\"])\n",
    "df_train_x.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle nan\n",
    "df_train_x[\"artist_id\"].fillna(0, inplace=True)\n",
    "df_train_x[\"language_id\"].fillna(0, inplace=True)\n",
    "df_train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data format to be model training friendly\n",
    "\n",
    "df = df_train_x.copy()\n",
    "\n",
    "# step1 - data cleaning\n",
    "df[\"play_duration\"] = df[\"play_duration\"].dt.total_seconds()  # 將 play_duration 轉換為秒\n",
    "df[\"play_duration\"].fillna(0, inplace=True)\n",
    "\n",
    "# # step2 - feature engineering\n",
    "# # 為每首歌創建特徵（假設 cols 是相關列）\n",
    "features = [\"song_index\", \"artist_id\", \"play_duration\"]\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for i in range(1, 21):  # 迴圈處理 20 首歌\n",
    "    for feature in features:\n",
    "        user_feature_name = f\"{feature}_{i}\"\n",
    "        user_feature_values = df[df[\"listening_order\"] == i][feature].values\n",
    "        new_df[user_feature_name] = user_feature_values\n",
    "\n",
    "train_x = new_df\n",
    "train_x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_21 = train_target[train_target[\"listening_order\"] == 21].reset_index()\n",
    "train_y = mask_21[[\"song_index\"]]\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use top10 playduration predict 1-> ramdomforest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_x is a DataFrame\n",
    "\n",
    "topk = 5\n",
    "train_x_k_rows = []\n",
    "\n",
    "# Iterate over rows using iterrows\n",
    "for idx, row in train_x.iterrows():\n",
    "    # Get topk play duration indices\n",
    "    playdurations = [row[f\"play_duration_{i}\"] for i in range(1, 21)]\n",
    "    topk_indices = sorted(\n",
    "        range(len(playdurations)), key=lambda i: playdurations[i], reverse=True\n",
    "    )[:topk]\n",
    "\n",
    "    # Create a new row dictionary\n",
    "    new_row = {}\n",
    "    for i, idx in enumerate(topk_indices, start=1):\n",
    "        new_row[f\"song_index_{i}\"] = row[f\"song_index_{idx + 1}\"]\n",
    "        new_row[f\"play_duration_{i}\"] = row[f\"play_duration_{idx + 1}\"]\n",
    "        new_row[f\"artist_id_{i}\"] = row[f\"artist_id_{idx + 1}\"]\n",
    "\n",
    "    # Append the new row to the list\n",
    "    train_x_k_rows.append(new_row)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "train_x_10 = pd.DataFrame(train_x_k_rows)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "train_x_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x, train_y, test_size=0.1, random_state=42\n",
    ")\n",
    "model = ensemble.RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "predict_01 = model.predict(X_train)\n",
    "# Calculate precision and recall from the confusion matrix\n",
    "precision = precision_score(\n",
    "    X_train, predict_01, average=\"weighted\"\n",
    ")  # 'weighted' for multiclass problems\n",
    "recall = recall_score(y_train, predict_01, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# df_plot = pd.DataFrame(\n",
    "#     {\"features\": train_x.columns[:], \"importances\": model.feature_importances_}\n",
    "# )\n",
    "# df_plot = df_plot.sort_values(\"importances\", ascending=False)\n",
    "# plt.figure(figsize=[11, 20])\n",
    "# sns.barplot(x=df_plot.importances, y=df_plot.features)\n",
    "# plt.title(\"Importances of Features Plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     train_x, train_y, test_size=0.95, random_state=42\n",
    "# )\n",
    "# model_01 = CatBoostClassifier(\n",
    "#     iterations=1,\n",
    "# )\n",
    "# model_01.fit(X_train, y_train[\"song_index\"])\n",
    "\n",
    "\n",
    "# predict_01 = model_01.predict(X_test)\n",
    "# # Generate the confusion matrix\n",
    "# cm1 = confusion_matrix(y_test, predict_01)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
