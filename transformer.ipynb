{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_root = Path(\"../kkdata3\")\n",
    "for x in data_root.glob(\"*\"):\n",
    "    print(x)\n",
    "\n",
    "train_source = pd.read_parquet(data_root / \"label_train_source.parquet\")\n",
    "train_target = pd.read_parquet(data_root / \"label_train_target.parquet\")\n",
    "test_source = pd.read_parquet(data_root / \"label_test_source.parquet\")\n",
    "meta_song = pd.read_parquet(data_root / \"meta_song.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source.sort_values([\"session_id\", \"listening_order\"], inplace=True)\n",
    "train_target.sort_values([\"session_id\", \"listening_order\"], inplace=True)\n",
    "test_source.sort_values([\"session_id\", \"listening_order\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map song_id to song_index to save memory and speed up\n",
    "meta_song[\"song_index\"] = meta_song.index\n",
    "train_source = train_source.merge(\n",
    "    meta_song[[\"song_id\", \"song_index\"]], on=\"song_id\", how=\"left\"\n",
    ")\n",
    "train_target = train_target.merge(\n",
    "    meta_song[[\"song_id\", \"song_index\"]], on=\"song_id\", how=\"left\"\n",
    ")\n",
    "test_source = test_source.merge(\n",
    "    meta_song[[\"song_id\", \"song_index\"]], on=\"song_id\", how=\"left\"\n",
    ")\n",
    "del train_source[\"song_id\"]\n",
    "del train_target[\"song_id\"]\n",
    "del test_source[\"song_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return n+1 column song id\n",
    "def getTrainData(df, n=2):\n",
    "    df = df.copy()\n",
    "    # gen n song id be the dataset\n",
    "    for i in range(1, n + 1):\n",
    "        df[f\"next{i}_song_id\"] = df[\"song_index\"].shift(-i)\n",
    "\n",
    "    # check if last song id is in the same session\n",
    "    df[f\"next{n}_session_id\"] = df[\"session_id\"].shift(-n)\n",
    "    df = df.query(f\"session_id == next{n}_session_id\")\n",
    "\n",
    "    # only get the song_id and next1_song_id, next2_song_id, next3_song_id... column\n",
    "    df = df[[\"song_index\"] + [f\"next{i}_song_id\" for i in range(1, n + 1)]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = getTrainData(train_source, n=19)\n",
    "trainY = getTrainData(train_target, n=4)\n",
    "\n",
    "trainX[\"song_index\"] = trainX[\"song_index\"].astype(\"float64\")\n",
    "trainY[\"song_index\"] = trainY[\"song_index\"].astype(\"float64\")\n",
    "\n",
    "trainX.reset_index(drop=True, inplace=True)\n",
    "trainY.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sos = meta_song[\"song_index\"].max() + 1\n",
    "eos = meta_song[\"song_index\"].max() + 2\n",
    "# get last 5 col of trainX\n",
    "src = trainX.iloc[:, -5:]\n",
    "src.insert(0, \"sos\", sos)\n",
    "src[\"eos\"] = eos\n",
    "\n",
    "# trainY concat with trainX\n",
    "tgt = trainY\n",
    "tgt.insert(0, \"sos\", sos)\n",
    "tgt[\"eos\"] = eos\n",
    "\n",
    "# check if src and tgt shape is the same\n",
    "src.shape == tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, source_data, target_data):\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tensor = torch.LongTensor(self.source_data.iloc[idx].values)\n",
    "        tgt_tensor = torch.LongTensor(self.target_data.iloc[idx].values)\n",
    "        return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "src, src_val, tgt, tgt_val = train_test_split(src, tgt, test_size=0.1, random_state=42)\n",
    "\n",
    "src_val = src_val[:16]\n",
    "tgt_val = tgt_val[:16]\n",
    "\n",
    "train_dataset = Dataset(source_data=src, target_data=tgt)\n",
    "validation_dataset = Dataset(source_data=src_val, target_data=tgt_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source[\"song_index\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 7\n",
    "VOCAB_SIZE = meta_song[\"song_index\"].max() + 3  # 1030711 + sos + eos + 0\n",
    "EMBEDDING_DIM = 32\n",
    "NHEAD = 8\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 5000,\n",
    "        batch_first: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.batch_first:\n",
    "            x = x.transpose(0, 1)\n",
    "            x = x + self.pe[: x.size(0)]\n",
    "            return self.dropout(x.transpose(0, 1))\n",
    "        else:\n",
    "            x = x + self.pe[: x.size(0)]\n",
    "            return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM, padding_idx=0)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=EMBEDDING_DIM,\n",
    "            nhead=NHEAD,\n",
    "            num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "            num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.pos_embedding = PositionalEncoding(\n",
    "            EMBEDDING_DIM, dropout=0.1, max_len=MAX_SEQ_LEN, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(EMBEDDING_DIM, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, tgt_mask):\n",
    "        _ = self.embedding(src)\n",
    "        src = self.pos_embedding(_)\n",
    "\n",
    "        _ = self.embedding(tgt)\n",
    "        tgt = self.pos_embedding(_)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "        )\n",
    "\n",
    "        return self.fc(output)\n",
    "\n",
    "\n",
    "# detect where the padding value is\n",
    "def gen_padding_mask(src, pad_idx=0.0):\n",
    "    # pad_mask = (src == pad_idx\n",
    "    return src.eq(pad_idx)\n",
    "\n",
    "\n",
    "# triu mask for decoder\n",
    "def gen_mask(seq):\n",
    "    seq_len = seq.size(1)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(pred, dim=2):\n",
    "    return pred.clone().argmax(dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred: list, target: list) -> float:\n",
    "    \"\"\"\n",
    "    pred: list of strings\n",
    "    target: list of strings\n",
    "\n",
    "    return: accuracy(%)\n",
    "    \"\"\"\n",
    "    if len(pred) != len(target):\n",
    "        raise ValueError(\"length of pred and target must be the same\")\n",
    "    correct = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == target[i]:\n",
    "            correct += 1\n",
    "    return correct / len(pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(validation_loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, model, device, logout=False, dataset=\"test\"):\n",
    "    pred_str_list = []\n",
    "    tgt_str_list = []\n",
    "    input_str_list = []\n",
    "    losses = []\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        # An all pad token tensor with the same shape as tgt and the first token is <sos>\n",
    "        tgt_input = torch.full_like(tgt, fill_value=0)  # 0 is the pad token\n",
    "        tgt_input[:, 0] = sos  # -1 is the <sos> token\n",
    "        for i in range(tgt.shape[1] - 1):\n",
    "            src_pad_mask = gen_padding_mask(src, pad_idx=0).to(device)\n",
    "            tgt_pad_mask = gen_padding_mask(tgt_input, pad_idx=0).to(device)\n",
    "            tgt_mask = gen_mask(tgt_input).to(device)\n",
    "            pred = model(\n",
    "                src=src,\n",
    "                tgt=tgt_input,\n",
    "                src_padding_mask=src_pad_mask,\n",
    "                tgt_padding_mask=tgt_pad_mask,\n",
    "                tgt_mask=tgt_mask,\n",
    "            )\n",
    "            pred_idx = get_index(pred)\n",
    "            tgt_input[:, i + 1] = pred_idx[:, i]\n",
    "\n",
    "        for i in range(tgt.shape[0]):\n",
    "            pred_str_list.append(tgt_input[i].tolist())\n",
    "            tgt_str_list.append(tgt[i].tolist())\n",
    "            input_str_list.append(src[i].tolist())\n",
    "            if logout:\n",
    "                print(\"=\" * 30)\n",
    "                print(f\"input: {input_str_list[-1]}\")\n",
    "                print(f\"pred: {pred_str_list[-1]}\")\n",
    "                print(f\"target: {tgt_str_list[-1]}\")\n",
    "        loss = ce_loss(pred[:, :-1, :].permute(0, 2, 1), tgt[:, 1:])\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "\n",
    "    print(\n",
    "        f\"{dataset}_acc: {metrics(pred_str_list, tgt_str_list):.2f}\",\n",
    "        f\"{dataset}_loss: {avg_loss:.2f}\",\n",
    "        end=\" | \",\n",
    "    )\n",
    "    print(f\"[pred: {pred_str_list[0]} target: {tgt_str_list[0]}]\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = torch.randint(\n",
    "#     0, VOCAB_SIZE, (100, MAX_SEQ_LEN), dtype=torch.long, device=device\n",
    "# )\n",
    "# train_y = torch.randint(\n",
    "#     0, VOCAB_SIZE, (100, MAX_SEQ_LEN), dtype=torch.long, device=device\n",
    "# )\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model = TransformerModel().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 20\n",
    "train_losses = []\n",
    "with torch.autograd.detect_anomaly():\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for train_x, train_y in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "            train_x, train_y = train_x.to(device=device), train_y.to(device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            src_pad_mask = gen_padding_mask(train_x).to(device)\n",
    "            tgt_pad_mask = gen_padding_mask(train_y).to(device)\n",
    "            tgt_mask = gen_mask(train_y).to(device)\n",
    "\n",
    "            output = model(\n",
    "                train_x,\n",
    "                train_y,\n",
    "                src_padding_mask=src_pad_mask,\n",
    "                tgt_padding_mask=tgt_pad_mask,\n",
    "                tgt_mask=tgt_mask,\n",
    "            )\n",
    "\n",
    "            # print(output[:, :-1, :].permute(0, 2, 1).shape, train_y[:, 1:].shape)\n",
    "\n",
    "            loss = criterion(output.permute(0, 2, 1), train_y)\n",
    "            # loss = criterion(output[:, :-1, :].permute(0, 2, 1), train_y[:, 1:])\n",
    "            # loss = criterion(output.reshape(-1, VOCAB_SIZE), train_y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 500 == 0:\n",
    "                train_losses.append(loss.item())\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(train_losses, label=\"Training Loss\")\n",
    "                plt.ylabel(\"CrossEntropy Loss\")\n",
    "                plt.title(\"Training Loss Curve\")\n",
    "                plt.savefig(\"result.png\")\n",
    "                # Close the figure to prevent it from being displayed\n",
    "                plt.close()\n",
    "            i += 1\n",
    "        # test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _ = validation(validation_loader, model, device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
